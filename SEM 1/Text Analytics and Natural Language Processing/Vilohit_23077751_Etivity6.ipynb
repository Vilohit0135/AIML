{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "## Student Name:- Vilohit Keshava Murthy Achar.\n",
        "## Student Id :- 23077751"
      ],
      "metadata": {
        "id": "zQ784OfUl9bg"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Task1:-"
      ],
      "metadata": {
        "id": "5I-w31GNl33I"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "Given the confusion matrices for two classes, Food and Drink, we calculate the following:\n",
        "\n",
        "\n",
        "{ Food:}\n",
        "\\begin{align}\n",
        "\\text{Precicion} &= P \\\\\n",
        "\\text{Recall} &= R \\\\\n",
        "\\text{True Positives (TP1)} &= 800 \\\\\n",
        "\\text{False Positives (FP1)} &= 200 \\\\\n",
        "\\text{False Negatives (FN1)} &= 200 \\\\\n",
        "\\text{P1} &= \\frac{TP1}{TP1 + FP1} = \\frac{800}{800 + 200} = 0.800 \\\\\n",
        "\\text{R1} &= \\frac{TP1}{TP1 + FN1} = \\frac{800}{800 + 200} = 0.800 \\\\\n",
        "\\text{F1 Score} &= 2 \\times \\frac{\\text{P1} \\times \\text{R1}}{\\text{P1} + \\text{R1}} = 2 \\times \\frac{0.800 \\times 0.800}{0.800 + 0.800} = 0.800\n",
        "\\end{align}\n",
        "\n",
        "{ Drink:}\n",
        "\\begin{align}\n",
        "\\text{True Positives (TP2)} &= 70 \\\\\n",
        "\\text{False Positives (FP2)} &= 30 \\\\\n",
        "\\text{False Negatives (FN2)} &= 30 \\\\\n",
        "\\text{P2} &= \\frac{TP2}{TP2 + FP2} = \\frac{70}{70 + 30} = 0.700 \\\\\n",
        "\\text{R2} &= \\frac{TP2}{TP2 + FN2} = \\frac{70}{70 + 30} = 0.700 \\\\\n",
        "\\text{F1 Score} &= 2 \\times \\frac{\\text{P2} \\times \\text{R2}}{\\text{P2} + \\text{R2}} = 2 \\times \\frac{0.700 \\times 0.700}{0.700 + 0.700} = 0.700\n",
        "\\end{align}\n",
        "\n",
        "{Microaveraged Metrics:}\n",
        "\n",
        "For both groups, we add up the true positives, false positives, and false negatives:\n",
        "\\begin{align}\n",
        "\\text{Micro TP} &= TP_{\\text{Food}} + TP_{\\text{Drink}} = 800 + 70 = 870 \\\\\n",
        "\\text{Micro FP} &= FP_{\\text{Food}} + FP_{\\text{Drink}} = 200 + 30 = 230 \\\\\n",
        "\\text{Micro FN} &= FN_{\\text{Food}} + FN_{\\text{Drink}} = 200 + 30 = 230 \\\\\n",
        "\\text{Micro P} &= \\frac{\\text{Micro TP}}{\\text{Micro TP} + \\text{Micro FP}} = \\frac{870}{870 + 230} = 0.791 \\\\\n",
        "\\text{Micro R} &= \\frac{\\text{Micro TP}}{\\text{Micro TP} + \\text{Micro FN}} = \\frac{870}{870 + 230} = 0.791 \\\\\n",
        "\\text{Micro F1 Score} &= 2 \\times \\frac{\\text{Micro P} \\times \\text{Micro R}}{\\text{Micro P} + \\text{Micro R}} = 2 \\times \\frac{0.791 \\times 0.791}{0.791 + 0.791} = 0.791\n",
        "\\end{align}\n",
        "\n",
        "{Macroaveraged Metrics:-}\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "We take the mean of the measurements for each classes:\n",
        "\\begin{align}\n",
        "\\text{Macro P} &= \\frac{\\text{P}_{\\text{Food}} + \\text{P}_{\\text{Drink}}}{2} = \\frac{0.800 + 0.700}{2} = 0.750 \\\\\n",
        "\\text{Macro R} &= \\frac{\\text{R}_{\\text{Food}} + \\text{R}_{\\text{Drink}}}{2} = \\frac{0.800 + 0.700}{2} = 0.750 \\\\\n",
        "\\text{Macro F1 Score} &= \\frac{\\text{F1}_{\\text{Food}} + \\text{F1}_{\\text{Drink}}}{2} = \\frac{0.800 + 0.700}{2} = 0.750\n",
        "\\end{align}\n"
      ],
      "metadata": {
        "id": "nbeA3_86iAnT"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "- Microaveraged metrics calculate the average performance by summing the true positives, false negatives, and false positives across all categories, thereby considering the collective impact of all classifications.\n",
        "\n",
        "- On the other hand, macroaveraged metrics are determined by calculating the average of the individual metrics for each category, giving equal weight to each category irrespective of its frequency.The disparity between microaveraged and macroaveraged F1 scores stems from the fact that the microaverage is influenced by the category with more instances due to its cumulative approach to true positives, false positives, and false negatives. Conversely, the macroaverage assesses each category on an equal footing, leading to a potentially lower macroaverage if performance varies significantly between categories. In the provided example, the 'Food' category's larger number of instances elevates the microaveraged metrics above the macroaveraged metrics."
      ],
      "metadata": {
        "id": "8XkX2h7Om6He"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Task 2:-"
      ],
      "metadata": {
        "id": "1ylCXbj5n_IL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def naiveBayesClassifier(trainingSet,testSet):\n",
        "\n",
        "  megaDocGB = []\n",
        "  megaDocIE = []\n",
        "\n",
        "  VOC = {}\n",
        "  GB_BOW = {}\n",
        "  IE_BOW = {}\n",
        "\n",
        "  prob_gb = 0\n",
        "  prob_ie = 0\n",
        "\n",
        "  for doc, category in trainingSet:\n",
        "\n",
        "    if category == 'GB':\n",
        "      megaDocGB.append(doc)\n",
        "    else:\n",
        "      megaDocIE.append(doc)\n",
        "\n",
        "    for word in doc.lower().split():\n",
        "\n",
        "      VOC[word] = VOC.get(word, 0) + 1\n",
        "\n",
        "      if category == 'GB':\n",
        "          GB_BOW[word] = GB_BOW.get(word, 0) + 1\n",
        "      else:\n",
        "          IE_BOW[word] = IE_BOW.get(word, 0) + 1\n",
        "\n",
        "  megaDocGB = ' '.join(megaDocGB)\n",
        "  megaDocIE = ' '.join(megaDocIE)\n",
        "\n",
        "  print(\"megaDocGB= \", megaDocGB)\n",
        "  print(\"megaDocIE= \", megaDocIE)\n",
        "\n",
        "  prob_gb = sum(GB_BOW.values()) / (sum(GB_BOW.values()) + sum(IE_BOW.values()))\n",
        "  prob_ie = sum(IE_BOW.values()) / (sum(GB_BOW.values()) + sum(IE_BOW.values()))\n",
        "\n",
        "  print(f\"prob_gb={prob_gb}, prob_ie={prob_ie}\\n\")\n",
        "  print(f\"class_bow_gb={GB_BOW}\\n\")\n",
        "  print(f\"class_bow_ie={IE_BOW}\\n\")\n",
        "\n",
        "  print(f\"vocabulary={VOC}\\n\")\n",
        "  print(f\"|vocabulary|={len(VOC)}\\n\")\n",
        "\n",
        "  print(\"-\"*100)\n",
        "\n",
        "  CProbGB = {}\n",
        "  CProbIE = {}\n",
        "\n",
        "  for word in VOC:\n",
        "    CProbGB[word] = (GB_BOW.get(word, 0) + 1) / (sum(GB_BOW.values()) + len(VOC))\n",
        "    CProbIE[word] = (IE_BOW.get(word, 0) + 1) / (sum(IE_BOW.values()) + len(VOC))\n",
        "\n",
        "  for doc, question in testSet:\n",
        "\n",
        "    print('-'*100)\n",
        "    print(f\"Test Document=({doc}, '?')\");\n",
        "\n",
        "    for word in doc.lower().split():\n",
        "\n",
        "      if word in VOC:\n",
        "\n",
        "        prob_gb *= CProbGB[word]\n",
        "        prob_ie *= CProbIE[word]\n",
        "\n",
        "        print(f\"\\nWord={word}, wordConditionalProbGB = {CProbGB[word]},  wordConditionalProbIE = {CProbIE[word]}\")\n",
        "\n",
        "    if prob_gb > prob_ie:\n",
        "      inferred_class = 'GB'\n",
        "    else :\n",
        "      inferred_class = 'IE'\n",
        "\n",
        "    print(f\"\\ndocProbGB = {prob_gb},  docProbIE = {prob_ie}\\nInferred class = {inferred_class}\")\n",
        "\n",
        "trainingSet = [('just plain boring','-'),('entirely predictable and lacks energy','-'),('no surprises and very few laughs','-'),('very powerful','+'),('the most fun film of the summer','+')]\n",
        "testSet = [('predictable with no fun','?')]\n",
        "\n",
        "naiveBayesClassifier(trainingSet,testSet)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-VdUs9kyrWUS",
        "outputId": "617f8828-b9bf-405e-f7f6-1a8ea54ae819"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "megaDocGB=  \n",
            "megaDocIE=  just plain boring entirely predictable and lacks energy no surprises and very few laughs very powerful the most fun film of the summer\n",
            "prob_gb=0.0, prob_ie=1.0\n",
            "\n",
            "class_bow_gb={}\n",
            "\n",
            "class_bow_ie={'just': 1, 'plain': 1, 'boring': 1, 'entirely': 1, 'predictable': 1, 'and': 2, 'lacks': 1, 'energy': 1, 'no': 1, 'surprises': 1, 'very': 2, 'few': 1, 'laughs': 1, 'powerful': 1, 'the': 2, 'most': 1, 'fun': 1, 'film': 1, 'of': 1, 'summer': 1}\n",
            "\n",
            "vocabulary={'just': 1, 'plain': 1, 'boring': 1, 'entirely': 1, 'predictable': 1, 'and': 2, 'lacks': 1, 'energy': 1, 'no': 1, 'surprises': 1, 'very': 2, 'few': 1, 'laughs': 1, 'powerful': 1, 'the': 2, 'most': 1, 'fun': 1, 'film': 1, 'of': 1, 'summer': 1}\n",
            "\n",
            "|vocabulary|=20\n",
            "\n",
            "----------------------------------------------------------------------------------------------------\n",
            "----------------------------------------------------------------------------------------------------\n",
            "Test Document=(predictable with no fun, '?')\n",
            "\n",
            "Word=predictable, wordConditionalProbGB = 0.05,  wordConditionalProbIE = 0.046511627906976744\n",
            "\n",
            "Word=no, wordConditionalProbGB = 0.05,  wordConditionalProbIE = 0.046511627906976744\n",
            "\n",
            "Word=fun, wordConditionalProbGB = 0.05,  wordConditionalProbIE = 0.046511627906976744\n",
            "\n",
            "docProbGB = 0.0,  docProbIE = 0.00010062007118870036\n",
            "Inferred class = IE\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Task 3:-"
      ],
      "metadata": {
        "id": "RcoD6-cjmNIe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import math\n",
        "from textblob import TextBlob\n",
        "def SentimentAnalyzer(string):\n",
        "    sentence = TextBlob(string)\n",
        "    polarity = sentence.sentiment.polarity\n",
        "\n",
        "    if polarity > 0:\n",
        "        sentiment = \"postive sentiment ğŸ˜€\"\n",
        "    elif polarity < 0:\n",
        "        sentiment = \"negative sentiment ğŸ˜”\"\n",
        "    else:\n",
        "        sentiment = \"neural sentiment ğŸ˜\"\n",
        "\n",
        "    print(\"string=\",string)\n",
        "    print(\"\\n\",sentence.sentiment)\n",
        "    print(sentiment)\n",
        "    print(\"Subjectivity:\", sentence.sentiment.subjectivity)\n",
        "\n",
        "    print(\"-------------------------------------------------\")\n",
        "    return\n",
        "\n",
        "SentimentAnalyzer(\"NLP is cool\")\n",
        "SentimentAnalyzer(\"NLP is cool and useful\")\n",
        "SentimentAnalyzer(\"NLP is hard\")\n",
        "SentimentAnalyzer(\"NLP is hard and useless\")\n",
        "SentimentAnalyzer(\"NLP stands for Natural LanguageÂ Processing\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "962IKjeKmRUP",
        "outputId": "20a3dfa1-e742-420b-984f-abadb1292fb2"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "string= NLP is cool\n",
            "\n",
            " Sentiment(polarity=0.35, subjectivity=0.65)\n",
            "postive sentiment ğŸ˜€\n",
            "Subjectivity: 0.65\n",
            "-------------------------------------------------\n",
            "string= NLP is cool and useful\n",
            "\n",
            " Sentiment(polarity=0.32499999999999996, subjectivity=0.325)\n",
            "postive sentiment ğŸ˜€\n",
            "Subjectivity: 0.325\n",
            "-------------------------------------------------\n",
            "string= NLP is hard\n",
            "\n",
            " Sentiment(polarity=-0.2916666666666667, subjectivity=0.5416666666666666)\n",
            "negative sentiment ğŸ˜”\n",
            "Subjectivity: 0.5416666666666666\n",
            "-------------------------------------------------\n",
            "string= NLP is hard and useless\n",
            "\n",
            " Sentiment(polarity=-0.39583333333333337, subjectivity=0.37083333333333335)\n",
            "negative sentiment ğŸ˜”\n",
            "Subjectivity: 0.37083333333333335\n",
            "-------------------------------------------------\n",
            "string= NLP stands for Natural LanguageÂ Processing\n",
            "\n",
            " Sentiment(polarity=0.1, subjectivity=0.4)\n",
            "postive sentiment ğŸ˜€\n",
            "Subjectivity: 0.4\n",
            "-------------------------------------------------\n"
          ]
        }
      ]
    }
  ]
}